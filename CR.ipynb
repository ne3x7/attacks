{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR10, SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curvature_regularization_loss(model, lossf, x, h, y=None, attack_norm_p='inf'):\n",
    "    \"\"\"\n",
    "    Computes curvature regularization term.\n",
    "    \n",
    "    The formula is L(x) = \\| \\nabla l(x + h z) - \\nabla l(x) \\|^2,\n",
    "    where z depends on attack norm. If attack is in \\ell_inf, then\n",
    "    z = sign \\nabla l(x) / \\| sign \\nabla l(x) \\|. Another good\n",
    "    choice is z = \\nabla l(x) / \\| \\nabla l(x) \\|.\n",
    "    \n",
    "    Args:\n",
    "        model, lossf (Module): model and corresponding loss function\n",
    "        x, y (Tensor): data and optional label\n",
    "        h (float): interpolation parameter\n",
    "        attack_norm_p (str): if 'inf', \\ell_inf z is used, otherwise\n",
    "            simply normalized gradient.\n",
    "    \"\"\"\n",
    "    original = x.clone().detach()\n",
    "    prob_original = lossf(model(original), y) if y is not None else lossf(model(original))\n",
    "    gradients_original = torch.autograd.grad(outputs=prob_original,\n",
    "                                             inputs=original,\n",
    "                                             grad_outputs=torch.ones(prob_original.size()),\n",
    "                                             create_graph=True,\n",
    "                                             retain_graph=True)[0]\n",
    "    \n",
    "    # do not back-propagate through z\n",
    "    if attack_norm_p == 'inf':\n",
    "        z = gradients_original.clone().detach().sign()\n",
    "    else:\n",
    "        z = gradients_original.clone().detach()\n",
    "    \n",
    "    interpolated = x + h * z\n",
    "    prob_interpolated = lossf(model(interpolated), y) if y is not None else lossf(model(interpolated))\n",
    "    gradients_interpolated = torch.autograd.grad(outputs=prob_interpolated,\n",
    "                                                 inputs=interpolated,\n",
    "                                                 grad_outputs=torch.ones(prob_interpolated.size()),\n",
    "                                                 create_graph=True,\n",
    "                                                 retain_graph=True)[0]\n",
    "\n",
    "    return torch.sum((gradients_interpolated - gradients_original) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10('../data/cifar10', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = resnet18(pretrained=True)\n",
    "#opt = torch.optim.Adam()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "epochs_burnin = 5\n",
    "lr_schedule = np.linspace(1e-6, 1e-4, epochs).flip()\n",
    "h_schedule = np.linspace(0, 1.5, epochs_burnin).tolist() + np.repeat([1.5], epochs-epochs_burnin)\n",
    "cr_weight = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, lossf, dataloader, lr_schedule, h_schedule, cr_weight, epochs=epochs):\n",
    "    for epoch, lr, h in zip(range(epochs), lr_schedule, h_schedule):\n",
    "        update_rate(optimizer, lr)\n",
    "        losses = []\n",
    "        for idx, (batch, labels) in enumerate(dataloader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = lossf(model(batch.cuda()), labels.cuda())\n",
    "            full_loss = loss + cr_weight * curvature_regularization_loss(model, lossf, batch.cuda(), h, labels.cuda())\n",
    "            losses.append(full_loss.detach().cpu().numpy())\n",
    "            full_loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print('[%2d]\\tloss\\t%.7f' % (epoch+1, np.mean(losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
