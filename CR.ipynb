{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curvature_regularization_loss(model, x, h, attack_norm_p='inf'):\n",
    "    \"\"\"\n",
    "    Computes curvature regularization term.\n",
    "    \n",
    "    The formula is L(x) = \\| \\nabla l(x + h z) - \\nabla l(x) \\|^2,\n",
    "    where z depends on attack norm. If attack is in \\ell_inf, then\n",
    "    z = sign \\nabla l(x) / \\| sign \\nabla l(x) \\|. Another good\n",
    "    choice is z = \\nabla l(x) / \\| \\nabla l(x) \\|.\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor): data point\n",
    "        h (float): interpolation parameter\n",
    "        attack_norm_p (str): if 'inf', \\ell_inf z is used\n",
    "    \"\"\"\n",
    "    original = torch.zeros_like(x).copy_(x)\n",
    "    prob_original = model(original)\n",
    "    gradients_original = torch.autograd.grad(outputs=prob_original,\n",
    "                                             inputs=original,\n",
    "                                             grad_outputs=torch.ones(prob_original.size()),\n",
    "                                             create_graph=True,\n",
    "                                             retain_graph=True)[0]\n",
    "    \n",
    "    if attack_norm_p == 'inf':\n",
    "        z = (gradients_original >= 0) * 1\n",
    "    \n",
    "    interpolated = x + h * z\n",
    "    prob_interpolated = model(interpolated)\n",
    "\n",
    "    gradients_interpolated = torch.autograd.grad(outputs=prob_interpolated,\n",
    "                                                 inputs=interpolated,\n",
    "                                                 grad_outputs=torch.ones(prob_interpolated.size()),\n",
    "                                                 create_graph=True,\n",
    "                                                 retain_graph=True)[0]\n",
    "\n",
    "    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "    # so flatten to easily take norm per example in batch\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
    "\n",
    "    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "    # the square root, so manually calculate norm and add epsilon\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "    # Return gradient penalty\n",
    "    return self.gp_weight * ((gradients_norm - 1) ** 2).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
